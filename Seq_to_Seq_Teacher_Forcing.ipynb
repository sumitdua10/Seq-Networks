{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq to Seq Teacher Forcing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitdua10/Seq-Networks/blob/master/Seq_to_Seq_Teacher_Forcing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pORSUqF1ExT-",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "59f49d3d-a24d-42f7-92f3-7476ab0a5f40"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-425815b6-05b5-4e99-b331-af69e3a40398\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-425815b6-05b5-4e99-b331-af69e3a40398\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving book2.txt to book2.txt\n",
            "User uploaded file \"book2.txt\" with length 20298 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xy_CnD7mE_uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ca5a8fa6-4dd6-467e-d2dc-2e790f56a9b2"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(fn, delimiter = '\\t')\n",
        "print(df.shape)\n",
        "print(df[:5])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(797, 4)\n",
            "           X  Date  Month  Year\n",
            "0  18/9/1970    18      9  1970\n",
            "1  15/7/1968    15      7  1968\n",
            "2  23/5/1963    23      5  1963\n",
            "3  27/7/2007    27      7  2007\n",
            "4  16/6/1995    16      6  1995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EEMXGr3WFKyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d27d4ad-c7f4-40ae-fee3-c031bafdb229"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM, GRU, Input, Embedding\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "\n",
        "#Code to get reproducible results\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# Force TensorFlow to use single thread.\n",
        "# Multiple threads are a potential source of non-reproducible results.\n",
        "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "\n",
        "from keras import backend as K\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZV2TBAoGdfrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "56e1087b-db78-4955-8280-68139394013f"
      },
      "cell_type": "code",
      "source": [
        "data = open(fn, 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "#print(data[1:50])\n",
        "\n",
        "data_size, VOCAB_LEN = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters(vocab length)in your data.' % (data_size, VOCAB_LEN))\n",
        "\n",
        "#2. Global variables\n",
        "MAX_INPUT_SEQ_LEN = len(max(df['X']))\n",
        "OUTPUT_SIZE = 4 # one for year, one for month and one for date\n",
        "RNN_SIZE = 96\n",
        "m = df.shape[0]\n",
        "\n",
        "print(\"Max Length \", MAX_INPUT_SEQ_LEN)\n",
        "\n",
        "#3. Create a dictionary & vocab size\n",
        "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
        "print(ix_to_char)\n",
        "print(char_to_ix)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 19501 total characters and 40 unique characters(vocab length)in your data.\n",
            "Max Length  29\n",
            "{0: '\\t', 1: '\\n', 2: ' ', 3: ',', 4: '-', 5: '/', 6: '0', 7: '1', 8: '2', 9: '3', 10: '4', 11: '5', 12: '6', 13: '7', 14: '8', 15: '9', 16: ':', 17: 'a', 18: 'b', 19: 'c', 20: 'd', 21: 'e', 22: 'f', 23: 'g', 24: 'h', 25: 'i', 26: 'j', 27: 'l', 28: 'm', 29: 'n', 30: 'o', 31: 'p', 32: 'r', 33: 's', 34: 't', 35: 'u', 36: 'v', 37: 'w', 38: 'x', 39: 'y'}\n",
            "{'\\t': 0, '\\n': 1, ' ': 2, ',': 3, '-': 4, '/': 5, '0': 6, '1': 7, '2': 8, '3': 9, '4': 10, '5': 11, '6': 12, '7': 13, '8': 14, '9': 15, ':': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'l': 27, 'm': 28, 'n': 29, 'o': 30, 'p': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'v': 36, 'w': 37, 'x': 38, 'y': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4JexW0HoVpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#5. Create on hot encoding of size x = m X SEQ_L X VOCAB_LEN and y = m X OUTPUT_SIZE * VOCAB_LEN\n",
        "\n",
        "x = np.zeros(shape = (m, MAX_INPUT_SEQ_LEN, VOCAB_LEN))\n",
        "y = np.zeros(shape=(m,OUTPUT_SIZE, VOCAB_LEN))\n",
        "\n",
        "#decoder input. This will be used as Teacher enforcing during training. \n",
        "# It will be y shifted by one character so that y[0] goes as input to 2nd decoder cell and so on. First input to decoder cell will be 0\n",
        "# and output h,c from encoder LSTM will be used as initial states for first cell of decoder model.\n",
        "\n",
        "de_x  = np.zeros(shape=(m,OUTPUT_SIZE, VOCAB_LEN))\n",
        "\n",
        "df['X'] = df['X'].str.lower()\n",
        "for i in range(m):\n",
        "    for j,k in enumerate(df['X'][i]):\n",
        "      try:\n",
        "        #print(k)\n",
        "        x[i][j][char_to_ix[k]] = 1\n",
        "      except:\n",
        "        print(\"i {} j {} k {}\".format( i, j, k))\n",
        "        \n",
        "    for a,b in enumerate(str(df['Year'][i])):\n",
        "      try:\n",
        "        \n",
        "        y[i][a][char_to_ix[b]] = 1\n",
        "        \n",
        "        \n",
        "        if a<OUTPUT_SIZE-1:\n",
        "          de_x[i][a+1][char_to_ix[b]] = 1\n",
        "        \n",
        "      except:\n",
        "        print(\"i {} a {} b {}\".format( i, a, b))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7NuHaH0rwQi8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define the layers and global variables\n",
        "\n",
        "\n",
        "activation_size=96\n",
        "activation_funct = 'tanh'\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "def seq_layer(ret_seq,  inputs):\n",
        "  return LSTM(activation_size,  activation=activation_funct, return_state=True, return_sequences = ret_seq)(inputs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wyzbx46l5qCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "223221d7-9f09-4ac4-d9c9-ddc98efd30fe"
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "en_inputs = Input(shape=(MAX_INPUT_SEQ_LEN, VOCAB_LEN))\n",
        "\n",
        "#encoder model will return only final state. so final tensor will represent the input text which will be further decoded.\n",
        "o,h,c = seq_layer(ret_seq=False, inputs = en_inputs)\n",
        "\n",
        "en_states = [h,c]\n",
        "\n",
        "#decoder inputs. it should have the seq length of output tensor.\n",
        "de_inputs = Input(shape = (OUTPUT_SIZE, VOCAB_LEN))\n",
        "\n",
        "#decoder LSTM will have return seq True and no. of cells matching with the output length.\n",
        "o = LSTM(activation_size,return_sequences = True)(inputs =de_inputs, initial_state = [h,c])\n",
        "\n",
        "print(o)\n",
        "\n",
        "#final dense layer of softmax to predict the character among available characters of size VOCAB_LEN\n",
        "o = Dense(VOCAB_LEN, activation='softmax')(o)\n",
        "print(o)\n",
        "\n",
        "\n",
        "model = Model(inputs=[en_inputs, de_inputs], outputs=o)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "sgd = optimizers.Adam(lr=0.01)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"lstm_20/transpose_1:0\", shape=(?, ?, 96), dtype=float32)\n",
            "Tensor(\"dense_15/truediv:0\", shape=(?, 4, 40), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_52 (InputLayer)           (None, 29, 40)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_53 (InputLayer)           (None, 4, 40)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_19 (LSTM)                  [(None, 96), (None,  52608       input_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_20 (LSTM)                  (None, 4, 96)        52608       input_53[0][0]                   \n",
            "                                                                 lstm_19[0][1]                    \n",
            "                                                                 lstm_19[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 4, 40)        3880        lstm_20[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,096\n",
            "Trainable params: 109,096\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2tIMZWqzO2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8834d50d-e83f-40a9-b2c4-7532e86ba0ac"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit([x,de_x], y, epochs= 3, validation_split = 0.25)  \n",
        "#model.fit(x, y, epochs= 30, validation_split = 0.25) \n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 597 samples, validate on 200 samples\n",
            "Epoch 1/3\n",
            "597/597 [==============================] - 1s 1ms/step - loss: 0.0796 - acc: 0.9761 - val_loss: 0.1738 - val_acc: 0.9575\n",
            "Epoch 2/3\n",
            "597/597 [==============================] - 1s 1ms/step - loss: 0.0772 - acc: 0.9770 - val_loss: 0.1933 - val_acc: 0.9500\n",
            "Epoch 3/3\n",
            "597/597 [==============================] - 1s 1ms/step - loss: 0.0825 - acc: 0.9736 - val_loss: 0.1841 - val_acc: 0.9600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f50a6931358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "metadata": {
        "id": "5FiFJmbbwICH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prediction "
      ]
    },
    {
      "metadata": {
        "id": "KGb2H9N9qAHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da90d1f5-a8c9-4af1-d9e9-4644cebc91d9"
      },
      "cell_type": "code",
      "source": [
        "pred_l = ['23-Dec-1955', '24 Dec 1960', 'December 15 2003', 'January of 15 90', '1995 1 Apr', \n",
        "          '15/01/2000', '19 May 1999', '01-02-90', '2 October 1993', '25 05 83']\n",
        "\n",
        "pred_l = [x.lower() for x in pred_l]\n",
        "\n",
        "#print(pred_l)\n",
        "pred_m = len(pred_l)\n",
        "print(\"TOtal size of prediction list: \", pred_m)\n",
        "\n",
        "pred_x = np.zeros(shape = (pred_m, MAX_INPUT_SEQ_LEN, VOCAB_LEN))\n",
        "\n",
        "for i in range(len(pred_l)):\n",
        "    for j,k in enumerate(pred_l[i]):\n",
        "      try:\n",
        "        pred_x[i][j][char_to_ix[k]] = 1\n",
        "      except:\n",
        "        print(\"i {} j {} k {}\".format( i, j, k))\n",
        "        \n",
        " "
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOtal size of prediction list:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PTxlREaEGO14",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's define the prediction modelling\n",
        "\n",
        "Prediction will have two Modes - Encoder model and decoder model.\n",
        "\n",
        "Encoder model will predict the final tensor representing the input text.\n",
        "Predicted tensor from Encoder will be used as input for decoder model. \n",
        "\n",
        "First cell of decoder will have final states from encoder model and input as zero or <SOS> token. The prediction from first cell is our predicted character. The h,s from this cell will be used input to subsequent cell till desired length or <EOS> token is reached."
      ]
    },
    {
      "metadata": {
        "id": "pF92PSQuLGnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "23b54d59-7bfd-4d10-e47b-f07c25c9e3b4"
      },
      "cell_type": "code",
      "source": [
        "#Define the prediction encoder model. This model will have the same weights as of training encoder model\n",
        "encoder_model = Model(en_inputs, en_states)\n",
        "print(encoder_model.summary())"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_52 (InputLayer)        (None, 29, 40)            0         \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               [(None, 96), (None, 96),  52608     \n",
            "=================================================================\n",
            "Total params: 52,608\n",
            "Trainable params: 52,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "moQIJ6TUr6Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b12cd0ab-5d50-4cc5-ef7e-30ee0d324b1d"
      },
      "cell_type": "code",
      "source": [
        "#Tt turns out prediction encoder model's  layer of weights is same as of training model first layer of weights.\n",
        "#Keras does it automatically as it's using same inputs\n",
        "\n",
        "w = model.get_weights()\n",
        "en_w = encoder_model.get_weights()\n",
        "print(\"Training Model Weights:\")\n",
        "for i in range(len(w)):\n",
        "  print(w[i].shape)\n",
        "\n",
        "print(\"\\nPrediction Model Weights:\")\n",
        "for i in range(len(en_w)):\n",
        "  print(en_w[i].shape)\n",
        "\n",
        "print(\"\\nTraining Model Sample Weights from first Tensor:\")\n",
        "print(w[0][0][:4])\n",
        "  \n",
        "print(\"\\nPrediction Model Sample Weights from first Tensor:\")\n",
        "print(en_w[0][0][:4])"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model Weights:\n",
            "(40, 384)\n",
            "(96, 384)\n",
            "(384,)\n",
            "(40, 384)\n",
            "(96, 384)\n",
            "(384,)\n",
            "(96, 40)\n",
            "(40,)\n",
            "\n",
            "Prediction Model Weights:\n",
            "(40, 384)\n",
            "(96, 384)\n",
            "(384,)\n",
            "\n",
            "Training Model Sample Weights from first Tensor:\n",
            "[-0.07492594 -0.0961068   0.06139428  0.03893624]\n",
            "\n",
            "Prediction Model Sample Weights from first Tensor:\n",
            "[-0.07492594 -0.0961068   0.06139428  0.03893624]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_n7FfNSpjNEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "08828297-bb46-4942-e326-5f3332bd8a9d"
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(1,VOCAB_LEN,))\n",
        "decoder_state_input_h = Input(shape=(activation_size,))\n",
        "decoder_state_input_c = Input(shape=(activation_size,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = LSTM (activation_size, return_state = True, return_sequences=False)(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = Dense(VOCAB_LEN, activation='softmax')(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],# decoder_states_inputs],\n",
        "    [decoder_outputs, state_h, state_c])#decoder_states])\n",
        "\n",
        "print(decoder_model.summary())\n",
        "  \n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_57 (InputLayer)           (None, 1, 40)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_58 (InputLayer)           (None, 96)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_59 (InputLayer)           (None, 96)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_22 (LSTM)                  [(None, 96), (None,  52608       input_57[0][0]                   \n",
            "                                                                 input_58[0][0]                   \n",
            "                                                                 input_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 40)           3880        lstm_22[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 56,488\n",
            "Trainable params: 56,488\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6CNErFdGjghH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d905feba-59e5-4146-cf69-2aed2f70a6eb"
      },
      "cell_type": "code",
      "source": [
        "#Tt turns out prediction decoders model's layers of weights is same as of training model's decoder layer of weights.\n",
        "#Keras does it automatically as it's using same inputs\n",
        "\n",
        "w = model.get_weights()\n",
        "de_w = decoder_model.get_weights()\n",
        "print(\"Training Model Weights:\")\n",
        "for i in range(len(w)):\n",
        "  print(\"Layer \",i, \": \", w[i].shape)\n",
        "\n",
        "\n",
        "print(\"\\nPrediction Model Weights:\")\n",
        "for i in range(len(de_w)):\n",
        "  print(\"Layer \",i, \": \", de_w[i].shape)\n",
        "\n",
        "print(\"\\nTraining Model Sample Weights from last Tensor:\")\n",
        "print(\"Layer last\", \": \", w[-1][:4])\n",
        "  \n",
        "print(\"\\nPrediction Model Sample Weights from last Tensor:\")\n",
        "print(\"Layer last\", \": \", de_w[-1][:4])\n",
        "decoder_model.set_weights(w[3:])\n",
        "\n",
        "print(\"\\nTraining Model Sample Weights from last Tensor:\")\n",
        "print(\"Layer last\", \": \", w[-1][:4])\n",
        "\n",
        "de_w = decoder_model.get_weights()\n",
        "print(\"\\nPrediction Model Sample Weights from last Tensor:\")\n",
        "print(\"Layer last\", \": \", de_w[-1][:4])\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model Weights:\n",
            "Layer  0 :  (40, 384)\n",
            "Layer  1 :  (96, 384)\n",
            "Layer  2 :  (384,)\n",
            "Layer  3 :  (40, 384)\n",
            "Layer  4 :  (96, 384)\n",
            "Layer  5 :  (384,)\n",
            "Layer  6 :  (96, 40)\n",
            "Layer  7 :  (40,)\n",
            "\n",
            "Prediction Model Weights:\n",
            "Layer  0 :  (40, 384)\n",
            "Layer  1 :  (96, 384)\n",
            "Layer  2 :  (384,)\n",
            "Layer  3 :  (96, 40)\n",
            "Layer  4 :  (40,)\n",
            "\n",
            "Training Model Sample Weights from last Tensor:\n",
            "Layer last :  [-0.22116935 -0.2755494  -0.22704618 -0.2136446 ]\n",
            "\n",
            "Prediction Model Sample Weights from last Tensor:\n",
            "Layer last :  [0. 0. 0. 0.]\n",
            "\n",
            "Training Model Sample Weights from last Tensor:\n",
            "Layer last :  [-0.22116935 -0.2755494  -0.22704618 -0.2136446 ]\n",
            "\n",
            "Prediction Model Sample Weights from last Tensor:\n",
            "Layer last :  [-0.22116935 -0.2755494  -0.22704618 -0.2136446 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZqPXCPeOE0GT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dccaf56b-f5c9-418e-c5ad-14a9940cee46"
      },
      "cell_type": "code",
      "source": [
        "#Final state prediction from encoder prediction model\n",
        "\n",
        "#encoder_model and get final state values\n",
        "states = encoder_model.predict(pred_x)\n",
        "\n",
        "#We deinfed output of prediction encoder model as en_states = [h,c] hence we get output as list of h & c\n",
        "pred_h = states[0]\n",
        "pred_c = states[1]\n",
        "\n",
        "pred_de_x  = np.zeros(shape=(OUTPUT_SIZE, pred_m,1, VOCAB_LEN))\n",
        "\n",
        "for i in range(OUTPUT_SIZE):\n",
        "  \n",
        "  de_o,de_h,de_c = decoder_model.predict([pred_de_x[0], pred_h, pred_c])#+states)\n",
        "  \n",
        "  #print(de_o.shape)\n",
        "  \n",
        "  pred_h = de_h\n",
        "  pred_c = de_c\n",
        "  \n",
        "  if i<OUTPUT_SIZE-1:\n",
        "    pred_de_x[i+1,:,0,:] = de_o\n",
        "  \n",
        "  pos = np.argmax(de_o, axis=1)\n",
        "  #print(pos)\n",
        "  out = [ix_to_char[i] for i in pos]\n",
        "  print(out)\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '1', '2', '2', '1', '2', '1', '2', '1', '2']\n",
            "['9', '9', '0', '0', '9', '0', '9', '0', '9', '0']\n",
            "['9', '6', '0', '1', '9', '0', '9', '0', '2', '0']\n",
            "['9', '2', '3', '9', '5', '0', '9', '3', '3', '0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tmCgzgPyT39i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}